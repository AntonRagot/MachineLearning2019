# PLAN

- [x] Finir de clean le data et le faire une fois pour tout (gros data set + test)
- [ ] Générer les word embeddings avec le truc donné: soit décider d'utiliser une taille fixe (200 dimensions) ou tester avec plusieurs (p.ex. 100,200,300)
- [x] Setup google colab
- [ ] Séléctionner modèles et se les répartir: decision tree / forest, naive bayes, LSTM, GRU, logisitc regression, CNN, SVM, MLP
- [ ] Combiner meilleurs résultats pour faire un _ensemble classifier_ avec majority vote
- [x] Se mettre d'accord sur une structure du repo
- [ ] Commencer le rapport (?)

# TODO:

## PREPROCESSING (ROBINCH)
- [ ] Exclamation mark
- [ ] ...
- [ ] ....

 ## EMBEDDING
 - [ ] Essayer de faire un embedding hybride (embedding de twitter + le notre pour les mots inconnus)
 
 ## MODELS
- [ ] decision tree / forest
- [ ] naive bayes
- [ ] LSTM
- [ ] GRU
- [ ] logisitc regression
- [ ] CNN
- [ ] SVM
- [ ] MLP
- [ ] recurrent convolutional network 

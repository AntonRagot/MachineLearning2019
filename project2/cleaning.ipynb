{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, test=False):\n",
    "    lines = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [l.strip() for l in lines]\n",
    "    if test:\n",
    "        ids = [l.split(',')[0] for l in lines]\n",
    "        return ids, lines\n",
    "    else:\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = load_data('data/train_pos.txt')\n",
    "neg = load_data('data/train_neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean(tweet):\n",
    "    lower = tweet.lower()\n",
    "    no_user_token = lower.replace('<user>', '')\n",
    "    no_token = no_user_token.replace('<url>', '')\n",
    "    no_punct = no_token.translate(str.maketrans('', '', string.punctuation))\n",
    "    no_digit = [w for w in no_punct.split() if not w.isdigit()]\n",
    "    return no_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pos = [clean(t) for t in pos]\n",
    "clean_neg = [clean(t) for t in neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize(words):\n",
    "    lem = WordNetLemmatizer()\n",
    "    verbs = [lem.lemmatize(w, 'v') for w in words] # verbs\n",
    "    return [lem.lemmatize(w, 'n') for w in verbs] # nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_pos = [lemmatize(w) for w in clean_pos]\n",
    "lemmatized_neg = [lemmatize(w) for w in clean_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned(path, data):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for words in data:\n",
    "            f.write(' '.join(words))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_cleaned('data/clean_train_pos.txt', lemmatized_pos)\n",
    "save_cleaned('data/clean_train_neg.txt', lemmatized_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter_lem_pos = Counter()\n",
    "counter_lem = Counter()\n",
    "for t in lemmatized_pos:\n",
    "    for w in t:\n",
    "        counter_lem_pos[w]+=1\n",
    "        counter_lem[w]+=1\n",
    "counter_lem_neg = Counter()\n",
    "for t in lemmatized_neg:\n",
    "    for w in t:\n",
    "        counter_lem_neg[w]+=1\n",
    "        counter_lem[w]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3330"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_lem_counter = {x : counter_lem[x] for x in counter_lem if counter_lem[x] > 50}\n",
    "len(clean_lem_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common in both\n",
    "thresh = 100\n",
    "most_common_pos = [w for w,c in counter_lem_pos.most_common()[:thresh]]\n",
    "most_common_neg = [w for w,c in counter_lem_neg.most_common()[:thresh]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_in_both = []\n",
    "for w in most_common_pos:\n",
    "    if w in most_common_neg:\n",
    "        most_common_in_both.append(w)\n",
    "#most_common_in_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove most common in both\n",
    "clean_lem_counter_2 = counter_lem.copy()\n",
    "for w in most_common_in_both:\n",
    "    clean_lem_counter_2.pop(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3255"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove words that appear scarcely\n",
    "thresh_2 = 50\n",
    "chosen_words = [x for x in clean_lem_counter_2 if clean_lem_counter_2[x] > thresh_2]\n",
    "len(chosen_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save words\n",
    "with open('data/chosen_words.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in chosen_words:\n",
    "        f.write(w + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
